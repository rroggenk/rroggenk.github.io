[
  {
    "objectID": "course_projects/2022-01-01-stat323/index.html",
    "href": "course_projects/2022-01-01-stat323/index.html",
    "title": "STAT 323: Design and Analysis of Experiments I",
    "section": "",
    "text": "test1\n\nIf the PDF does not load, click here to download it."
  },
  {
    "objectID": "541portfolio.html",
    "href": "541portfolio.html",
    "title": "STAT 541 Portfolio",
    "section": "",
    "text": "This page features projects from STAT 541: Advanced Statistical Computing with R, a 4-unit graduate course covering high-level programming techniques in R for statistical analysis and software development. Topics included version control, dynamic visualizations, functional programming, recursion, and efficient handling of large datasets. Each lab highlights a different skill or tool explored throughout the course.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 1: Quarto Warmup & STAT 331 Review\n\n\n\nQuarto\n\n\nR\n\n\n\nReviewing Quarto, ggplot and dplyr & Getting to know your classmates\n\n\n\nRachel Roggenkemper\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 2: Advanced Data Visualization\n\n\n\nQuarto\n\n\nR\n\n\n\nIdentifying Bad Visualizations and Broad Visualization Improvement\n\n\n\nRachel Roggenkemper\n\n\nApr 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 3: Static Dashboard\n\n\n\nQuarto\n\n\nR\n\n\nDashboards\n\n\n\nTransforming a Report into a Dashboard\n\n\n\nRachel Roggenkemper\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 4: Interactive Dashboard\n\n\n\nQuarto\n\n\nR\n\n\nDashboards\n\n\n\nTransforming a Static Dashboard into an Interactive Dashboard\n\n\n\nRachel Roggenkemper\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 6: Code Efficiency\n\n\n\nQuarto\n\n\nR\n\n\n\nPerforming Many Different Versions of an Analysis\n\n\n\nRachel Roggenkemper\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 7: Using APIs\n\n\n\nQuarto\n\n\nR\n\n\n\nVisualizing International Space Station Information via APIs\n\n\n\nRachel Roggenkemper, Liam Quach, and Lily Cook\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 8: Web Scraping\n\n\n\nQuarto\n\n\nR\n\n\n\nCheese Gromit!\n\n\n\nRachel Roggenkemper, Liam Quach, and Lily Cook\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 9: Generative Art\n\n\n\nQuarto\n\n\nR\n\n\n\nArt Gallery\n\n\n\nRachel Roggenkemper\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "541portfolio/2025-05-12-lab-06/index.html",
    "href": "541portfolio/2025-05-12-lab-06/index.html",
    "title": "Lab 6: Code Efficiency",
    "section": "",
    "text": "Note: I did use AI to help me think of scenarios where I needed to add warnings for my functions. I was able to account for the scenarios that are provided in the tests for the functions, but to make the functions more applicable, I wanted it to be able to handle additional scenarios (like if the user inputs a column that doesn’t exist in the dataframe).\nThis assignment will challenge your function writing abilities. I’m not going to lie, these functions are difficult but well within your reach. I do, however, want to recognize that not everyone is interested in being a “virtuoso” with their function writing. So, there are two options for this week’s lab:\nlibrary(tidyverse)"
  },
  {
    "objectID": "541portfolio/2025-05-12-lab-06/index.html#testing-your-function",
    "href": "541portfolio/2025-05-12-lab-06/index.html#testing-your-function",
    "title": "Lab 6: Code Efficiency",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\n## Testing how your function handles multiple input variables\nremove_outliers(diamonds, \n                price, \n                x, \n                y, \n                z)\n\n# A tibble: 52,689 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 52,679 more rows\n\n## Testing how your function handles an input that isn't numeric\nremove_outliers(diamonds, \n                price, \n                color)\n\n# A tibble: 52,734 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 52,724 more rows\n\n## Testing how your function handles a non-default sd_thresh\nremove_outliers(diamonds, \n                price,\n                x, \n                y, \n                z, \n                sd_thresh = 2)\n\n# A tibble: 50,099 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 50,089 more rows\n\n\nExercise 2: Write a function that imputes missing values for numeric variables in a dataset. The user should be able to supply the dataset, the variables to impute values for, and a function to use when imputing. Hint 1: You will need to use across() to apply your function, since the user can input multiple variables. Hint 2: The replace_na() function is helpful here!\n\n#' Imputes missing values in specified numeric columns using a summary function.\n#'\n#' @param df A data frame.\n#' @param ... Unquoted column names to impute.\n#' @param impute_fun A function like `mean` or `median` used to compute replacement values. Default is `mean`.\n#' @return A data frame where NAs in the selected numeric columns are replaced.\n\n\nimpute_missing &lt;- function(df, ..., impute_fun = mean) {\n  # Capture the unquoted col names\n  vars_to_impute &lt;- rlang::enquos(...)\n\n  # If the user doesn’t pass any vars, just return the og data\n  if (length(vars_to_impute) == 0) {\n    warning(\"No variables specified for imputation. Returning original data.\")\n    return(df)\n  }\n\n  # Extract the user given col names \n  selected_col_names &lt;- purrr::map_chr(vars_to_impute, rlang::as_name)\n\n  # Only keep columns that actually exist in the data\n  existing_selected_cols &lt;- selected_col_names[selected_col_names %in% names(df)]\n  non_existing_cols &lt;- setdiff(selected_col_names, existing_selected_cols)\n\n  # Warn if any user-specified cols weren’t found\n  if (length(non_existing_cols) &gt; 0) {\n    warning(\"These columns don’t exist in the data and will be skipped: \",\n            paste(non_existing_cols, collapse = \", \"))\n  }\n\n  # From the valid cols, keep only the numeric ones\n  if (length(existing_selected_cols) &gt; 0) {\n    numeric_cols_to_process &lt;- df %&gt;%\n      select(any_of(existing_selected_cols)) %&gt;%\n      select(where(is.numeric)) %&gt;%\n      colnames()\n  } else {\n    numeric_cols_to_process &lt;- character(0)\n  }\n\n  # Warn if any of the selected cols aren’t numeric\n  non_numeric_selected &lt;- setdiff(existing_selected_cols, numeric_cols_to_process)\n  if (length(non_numeric_selected) &gt; 0) {\n    warning(\"These columns are not numeric and will be ignored: \",\n            paste(non_numeric_selected, collapse = \", \"))\n  }\n\n  # If there’s nothing numeric to process, just return the og data\n  if (length(numeric_cols_to_process) == 0) {\n    if (length(existing_selected_cols) &gt; 0 && length(non_existing_cols) &lt; length(selected_col_names)) {\n      warning(\"No numeric columns found among selected variables. Returning original data.\")\n    }\n    return(df)\n  }\n\n  # Go through each selected numeric col and fill in the missing vals\n  df_imputed &lt;- df %&gt;%\n    mutate(\n      across(\n        all_of(numeric_cols_to_process),\n        ~ {\n          current_col_name &lt;- cur_column()\n          valid_values &lt;- .[!is.na(.)]\n\n          # Start with NA in case we can’t compute a valid value\n          imputation_value &lt;- NA\n\n          # If the whole column is NA, we can't impute anything, so leave it as is\n          if (length(valid_values) == 0) {\n            imputation_value &lt;- NA\n          } else {\n            # Try applying the user-specified function to get a value\n            temp_imputation_value &lt;- tryCatch(\n              impute_fun(valid_values),\n              error = function(e) {\n                warning(paste0(\"Problem applying impute_fun to '\", current_col_name, \n                               \"': \", e$message, \". Leaving NAs as-is.\"))\n                return(NA)\n              }\n            )\n\n            # Only keep the result if it’s one number / a single value\n            # Gemmy advised to add this step \n            if (length(temp_imputation_value) == 1 && is.atomic(temp_imputation_value)) {\n              imputation_value &lt;- temp_imputation_value\n            } else {\n              warning(paste0(\"Imputation function for '\", current_col_name, \n                             \"' didn’t return a single value. Leaving NAs as-is.\"))\n              imputation_value &lt;- NA\n            }\n          }\n\n          # Fill in missing values with the computed imputation value\n          tidyr::replace_na(., imputation_value)\n        }\n      )\n    )\n\n  return(df_imputed)\n}"
  },
  {
    "objectID": "541portfolio/2025-05-12-lab-06/index.html#testing-your-function-1",
    "href": "541portfolio/2025-05-12-lab-06/index.html#testing-your-function-1",
    "title": "Lab 6: Code Efficiency",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\n## Testing how your function handles multiple input variables\nimpute_missing(nycflights13::flights, \n               arr_delay, \n               dep_delay) \n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n## Testing how your function handles an input that isn't numeric\nimpute_missing(nycflights13::flights, \n               arr_delay, \n               carrier)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n## Testing how your function handles a non-default impute_fun\nimpute_missing(nycflights13::flights, \n               arr_delay, \n               dep_delay, \n               impute_fun = median)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "541portfolio/2025-05-12-lab-06/index.html#testing-your-function-2",
    "href": "541portfolio/2025-05-12-lab-06/index.html#testing-your-function-2",
    "title": "Lab 6: Code Efficiency",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\nfit_model(\n  diamonds,\n  mod_formula = price ~ carat + cut,\n  remove_outliers = TRUE,\n  impute_missing = TRUE,\n  price, \n  carat\n)\n\n\nCall:\nstats::lm(formula = mod_formula, data = processed_df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2460.16      7526.96      1059.65      -410.54       295.80        82.62"
  },
  {
    "objectID": "541portfolio/2025-05-12-lab-06/index.html#parameters",
    "href": "541portfolio/2025-05-12-lab-06/index.html#parameters",
    "title": "Lab 6: Code Efficiency",
    "section": "Parameters",
    "text": "Parameters\nFirst, we need to define the set of parameters we want to iterate the fit_model() function over. The tidyr package has a useful function called crossing() that is useful for generating argument combinations. For each argument, we specify all possible values for that argument and crossing() generates all combinations. Note that you can create a list of formula objects in R with c(y ~ x1, y ~ x1 + x2).\n\ndf_arg_combos &lt;- crossing(\n    impute = c(TRUE, FALSE),\n    remove_outliers = c(TRUE, FALSE), \n    mod = c(y ~ x1, \n            y ~ x1 + x2)\n)\ndf_arg_combos\n\nExercise 4: Use crossing() to create the data frame of argument combinations for our analyses.\n\n# Step 1: Set up the different model formulas \n# Each one adds more predictors to see how the model changes\nmodel_formulas &lt;- list(\n  price ~ carat,\n  price ~ carat + cut,\n  price ~ carat + cut + clarity,\n  price ~ carat + cut + clarity + color\n)\n\n# Step 2: Decide whether or not to impute missing values\n# Test both options: with and without imputation\nimputation_options &lt;- c(TRUE, FALSE)\n\n# Step 3: Decide whether or not to remove outliers\n# Test both options: with and without removing outliers\noutlier_removal_options &lt;- c(TRUE, FALSE)\n\n# Step 4: Create all possible combinations of the above choices\n# crossing() makes a data frame with every combo of formula, impute, and outlier setting\n# These column names match the argument names used in the fit_model() function\ndf_arg_combos &lt;- tidyr::crossing(\n  mod_formula = model_formulas,\n  impute_missing = imputation_options,\n  remove_outliers = outlier_removal_options\n)\n\n# Step 5: Take a look at the combinations created\n# There should be 4 formulas × 2 impute options × 2 outlier options = 16 total\nprint(df_arg_combos)\n\n# A tibble: 16 × 3\n   mod_formula impute_missing remove_outliers\n   &lt;list&gt;      &lt;lgl&gt;          &lt;lgl&gt;          \n 1 &lt;formula&gt;   FALSE          FALSE          \n 2 &lt;formula&gt;   FALSE          TRUE           \n 3 &lt;formula&gt;   TRUE           FALSE          \n 4 &lt;formula&gt;   TRUE           TRUE           \n 5 &lt;formula&gt;   FALSE          FALSE          \n 6 &lt;formula&gt;   FALSE          TRUE           \n 7 &lt;formula&gt;   TRUE           FALSE          \n 8 &lt;formula&gt;   TRUE           TRUE           \n 9 &lt;formula&gt;   FALSE          FALSE          \n10 &lt;formula&gt;   FALSE          TRUE           \n11 &lt;formula&gt;   TRUE           FALSE          \n12 &lt;formula&gt;   TRUE           TRUE           \n13 &lt;formula&gt;   FALSE          FALSE          \n14 &lt;formula&gt;   FALSE          TRUE           \n15 &lt;formula&gt;   TRUE           FALSE          \n16 &lt;formula&gt;   TRUE           TRUE"
  },
  {
    "objectID": "541portfolio/2025-05-12-lab-06/index.html#iterating-over-the-parameters",
    "href": "541portfolio/2025-05-12-lab-06/index.html#iterating-over-the-parameters",
    "title": "Lab 6: Code Efficiency",
    "section": "Iterating Over the Parameters",
    "text": "Iterating Over the Parameters\nWe’ve arrived at the final step!\nExercise 5: Use pmap() from purrr to apply the fit_model() function to every combination of arguments from `diamonds.\n\n# Apply fit_model() to each row of df_arg_combos\n# This will run all the combinations of formula, outlier removal, and imputation\n\nmessage(\"Starting model fitting for \", nrow(df_arg_combos), \" combinations...\")\n\ndf_results &lt;- df_arg_combos %&gt;%\n  mutate(\n    model_object = purrr::pmap(\n      # .l = list of arguments to vary; must match fit_model()’s argument names\n      .l = list(\n        mod_formula = mod_formula,\n        impute_missing = impute_missing,\n        remove_outliers = remove_outliers\n      ),\n      # .f = function to call on each row\n      .f = fit_model,\n      # These arguments are the same across all rows\n      df = diamonds,\n      carat  # Passed to ... inside fit_model; used for preprocessing\n    )\n  )\n\nmessage(\"Model fitting complete. Results are saved in 'df_results'.\")\n\n# Checking results:— each row should include the fitted model\nprint(df_results)\n\n# A tibble: 16 × 4\n   mod_formula impute_missing remove_outliers model_object\n   &lt;list&gt;      &lt;lgl&gt;          &lt;lgl&gt;           &lt;list&gt;      \n 1 &lt;formula&gt;   FALSE          FALSE           &lt;lm&gt;        \n 2 &lt;formula&gt;   FALSE          TRUE            &lt;lm&gt;        \n 3 &lt;formula&gt;   TRUE           FALSE           &lt;lm&gt;        \n 4 &lt;formula&gt;   TRUE           TRUE            &lt;lm&gt;        \n 5 &lt;formula&gt;   FALSE          FALSE           &lt;lm&gt;        \n 6 &lt;formula&gt;   FALSE          TRUE            &lt;lm&gt;        \n 7 &lt;formula&gt;   TRUE           FALSE           &lt;lm&gt;        \n 8 &lt;formula&gt;   TRUE           TRUE            &lt;lm&gt;        \n 9 &lt;formula&gt;   FALSE          FALSE           &lt;lm&gt;        \n10 &lt;formula&gt;   FALSE          TRUE            &lt;lm&gt;        \n11 &lt;formula&gt;   TRUE           FALSE           &lt;lm&gt;        \n12 &lt;formula&gt;   TRUE           TRUE            &lt;lm&gt;        \n13 &lt;formula&gt;   FALSE          FALSE           &lt;lm&gt;        \n14 &lt;formula&gt;   FALSE          TRUE            &lt;lm&gt;        \n15 &lt;formula&gt;   TRUE           FALSE           &lt;lm&gt;        \n16 &lt;formula&gt;   TRUE           TRUE            &lt;lm&gt;"
  },
  {
    "objectID": "541portfolio/2025-05-19-lab-07/index.html",
    "href": "541portfolio/2025-05-19-lab-07/index.html",
    "title": "Lab 7: Using APIs",
    "section": "",
    "text": "library(tidyverse)\nlibrary(httr)\nlibrary(tidyjson)\nlibrary(jsonlite)\nlibrary(leaflet)"
  },
  {
    "objectID": "541portfolio/2025-05-19-lab-07/index.html#api-functions",
    "href": "541portfolio/2025-05-19-lab-07/index.html#api-functions",
    "title": "Lab 7: Using APIs",
    "section": "API functions",
    "text": "API functions\n\n# function to get pass times for a single capital\n\nget_single_location_passes &lt;- function(lat, long) {\n  api_base_url &lt;- \"https://api.g7vrd.co.uk/v1/satellite-passes/25544\"\n  request_url &lt;- paste0(api_base_url, \"/\", lat, \"/\", long, \".json\")\n  \n  # get request\n  response &lt;- GET(request_url)\n  \n  # check if the request successful, if success get content\n  if (status_code(response) == 200) {\n    content &lt;- content(response, \"text\", encoding = \"UTF-8\")\n    pass_data &lt;- fromJSON(content)\n    \n    # extract time \n    if (length(pass_data$passes) &gt; 0 && \"tca\" %in% names(pass_data$passes)) {\n      # get first 3 TCA timestamps\n      return(head(pass_data$passes$tca, 3))\n    } else {\n      return(NULL) # if no passes found or tca field missing\n    }\n  } else {\n    warning(paste(\"API request failed for lat:\", lat, \"long:\", long, \"with status:\", status_code(response)))\n    return(NULL) # if request failed\n  }\n}\n\n# function to get pass times for capitals and create df\nget_all_capitals_passes &lt;- function(df) {\n  # empty list to store results\n  all_passes_list &lt;- list()\n  \n  # loop through each capital\n  for (i in 1:nrow(df)) {\n    capital_info &lt;- df[i, ]\n    \n    pass_times_vector &lt;- get_single_location_passes(lat = capital_info$lat, long = capital_info$long)\n    \n    # make tibble for the capital pass times\n    # three time columns, NA if less than 3 \n    current_passes_df &lt;- tibble(\n      state_abbr = capital_info$state,\n      capital_name = capital_info$capital,\n      lat = capital_info$lat,\n      long = capital_info$long,\n      time1 = if (length(pass_times_vector) &gt;= 1) pass_times_vector[1] else NA_character_,\n      time2 = if (length(pass_times_vector) &gt;= 2) pass_times_vector[2] else NA_character_,\n      time3 = if (length(pass_times_vector) &gt;= 3) pass_times_vector[3] else NA_character_\n    )\n    \n    all_passes_list[[i]] &lt;- current_passes_df\n    \n    # wait for 1 second between requests\n    Sys.sleep(1) \n  }\n  \n  # combine all tibbles\n  final_df &lt;- bind_rows(all_passes_list)\n  return(final_df)\n}\n\n\nPass Times Data\n\n# raw pass times for all capitals\niss_pass_data_raw &lt;- get_all_capitals_passes(capitals)\n\n# convert times and sort\niss_pass_data_processed &lt;- iss_pass_data_raw |&gt;\n  # convert UTC to datetime - help from AI\n  mutate(\n    time1_dt = ymd_hms(time1, tz = \"UTC\"),\n    time2_dt = ymd_hms(time2, tz = \"UTC\"),\n    time3_dt = ymd_hms(time3, tz = \"UTC\")\n  ) |&gt;\n  # arrange by first pass time\n  arrange(time1_dt) |&gt;\n  # remove rows where time 1 is NA\n  filter(!is.na(time1_dt))"
  },
  {
    "objectID": "541portfolio/2025-05-19-lab-07/index.html#mapping-the-data-and-drawing-the-iss-route",
    "href": "541portfolio/2025-05-19-lab-07/index.html#mapping-the-data-and-drawing-the-iss-route",
    "title": "Lab 7: Using APIs",
    "section": "2-5. Mapping the Data and Drawing the ISS Route",
    "text": "2-5. Mapping the Data and Drawing the ISS Route\n\nDefine Custom Icon\n\nsatellite_icon &lt;- makeIcon(\n  iconUrl = \"https://png.pngtree.com/png-clipart/20230111/original/pngtree-rocket-icon-vector-png-image_8902705.png\",\n  iconWidth = 25, \n  iconHeight = 25,\n  iconAnchorX = 12, \n  iconAnchorY = 12  \n)\n\n\n\nCreate the Leaflet Map\nWe construct the map layer by layer.\n\n# ensure there is data for plot\nif (nrow(iss_pass_data_processed) &gt; 0) {\n  \n  # format times (local time zone for readability) for labels etc\n  \n  map_data &lt;- iss_pass_data_processed |&gt;\n    mutate(\n      time1_display = format(time1_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      time2_display = format(time2_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      time3_display = format(time3_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      \n      # handle NA times in display strings\n      time2_display = ifelse(is.na(time2_dt), \"N/A\", time2_display),\n      time3_display = ifelse(is.na(time3_dt), \"N/A\", time3_display)\n    )\n\n  # hover labels\n  hover_labels &lt;- paste0(\n    \"&lt;strong&gt;Capital:&lt;/strong&gt; \", map_data$capital_name, \"&lt;br&gt;\",\n    \"&lt;strong&gt;State:&lt;/strong&gt; \", map_data$state_abbr, \"&lt;br&gt;\",\n    \"&lt;strong&gt;Soonest Pass:&lt;/strong&gt; \", map_data$time1_display\n  ) |&gt; lapply(htmltools::HTML) # lapply with HTML for proper rendering\n\n  # click popups\n  click_popups &lt;- paste0(\n    \"&lt;strong&gt;Capital:&lt;/strong&gt; \", map_data$capital_name, \" (\", map_data$state_abbr, \")&lt;br&gt;&lt;br&gt;\",\n    \"&lt;strong&gt;Predicted Pass Times (UTC):&lt;/strong&gt;&lt;br&gt;\",\n    \"1. \", map_data$time1_display, \"&lt;br&gt;\",\n    \"2. \", map_data$time2_display, \"&lt;br&gt;\",\n    \"3. \", map_data$time3_display\n  ) |&gt; lapply(htmltools::HTML)\n\n  # create the map\n  iss_map &lt;- leaflet(data = map_data) |&gt;\n    addTiles(group = \"OSM (Default)\") |&gt; # add default OpenStreetMap map tiles\n    addProviderTiles(providers$CartoDB.Positron, group = \"CartoDB Positron\") |&gt;\n    addProviderTiles(providers$Esri.WorldImagery, group = \"Esri World Imagery\") |&gt;\n    \n    # markers for each state capital\n    addMarkers(\n      lng = ~long, \n      lat = ~lat,\n      icon = satellite_icon,\n      label = hover_labels,\n      popup = click_popups,\n      group = \"State Capitals\"\n    ) |&gt;\n    \n    #  polylines connecting capitals in order of first pass time\n    addPolylines(\n      lng = ~long,\n      lat = ~lat,\n      color = \"#E6007E\", \n      weight = 3,\n      opacity = 0.8,\n      dashArray = \"5, 5\", # dashed line\n      group = \"ISS Pass Order Path\"\n    ) |&gt;\n    \n    #  layer controls to toggle layers\n    addLayersControl(\n      baseGroups = c(\"OSM (Default)\", \"CartoDB Positron\", \"Esri World Imagery\"),\n      overlayGroups = c(\"State Capitals\", \"ISS Pass Order Path\"),\n      options = layersControlOptions(collapsed = FALSE)\n    ) |&gt;\n    \n    # legend for the polyline\n    addLegend(\n        position = \"bottomright\",\n        colors = \"#E6007E\",\n        labels = \"ISS Pass Order Path\",\n        title = \"Map Features\"\n    )\n\n  # display the map\n  iss_map\n\n} else {\n  print(\"No ISS pass data available to map. Check API calls or data processing steps.\")\n}"
  },
  {
    "objectID": "541portfolio/2025-06-02-lab-09/index.html",
    "href": "541portfolio/2025-06-02-lab-09/index.html",
    "title": "Lab 9: Generative Art",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(RColorBrewer) \nlibrary(ambient) \nlibrary(purrr) \n\nset.seed(123)"
  },
  {
    "objectID": "541portfolio/2025-06-02-lab-09/index.html#set-up",
    "href": "541portfolio/2025-06-02-lab-09/index.html#set-up",
    "title": "Lab 9: Generative Art",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(RColorBrewer) \nlibrary(ambient) \nlibrary(purrr) \n\nset.seed(123)"
  },
  {
    "objectID": "541portfolio/2025-06-02-lab-09/index.html#art-piece-1",
    "href": "541portfolio/2025-06-02-lab-09/index.html#art-piece-1",
    "title": "Lab 9: Generative Art",
    "section": "Art Piece 1",
    "text": "Art Piece 1\n\n# Parameters\nnum_walks &lt;- 500\nnum_steps &lt;- 100\nstep_size_sd &lt;- 0.1\n\n# Generate random walk data\nwalk_data &lt;- map_df(1:num_walks, ~{\n  x_pos &lt;- 0\n  y_pos &lt;- 0\n  path_x &lt;- numeric(num_steps)\n  path_y &lt;- numeric(num_steps)\n  \n  for (i in 1:num_steps) {\n    x_pos &lt;- x_pos + rnorm(1, 0, step_size_sd)\n    y_pos &lt;- y_pos + rnorm(1, 0, step_size_sd)\n    path_x[i] &lt;- x_pos\n    path_y[i] &lt;- y_pos\n  }\n  \n  tibble(\n    walk_id = .x,\n    step_id = 1:num_steps,\n    x = path_x,\n    y = path_y\n  )\n})\n\n# Create the plot\nggplot(walk_data, aes(x = x, y = y, group = walk_id, color = factor(walk_id))) +\n  geom_path(alpha = 0.15, linewidth = 0.3) +\n  scale_color_manual(values = rep(brewer.pal(11, \"Spectral\"), length.out = num_walks)) +\n  theme_void() +\n  coord_fixed() +\n  theme(\n    legend.position = \"none\", # Hide legend for cleaner look\n    plot.background = element_rect(fill = \"black\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nTitle: Stochastic Bloom\nMuseum-Style Description:\nThis piece, Stochastic Bloom, captures the inherent tension between order and chaos that governs complex systems. Each luminous trajectory, born from a simple generative rule, embarks on a journey of apparent randomness. Yet, from this multitude of individual paths, a collective, almost organic structure emerges. The artist invites the viewer to contemplate the unseen algorithms that shape our world, from the flocking of birds to the ebb and flow of digital information. It is a meditation on emergence, where simplicity begets profound complexity, and data points transform into ethereal dance.\nDescription of the Code Choices and How They Influenced the Appearance:\n\nData Generation: The piece uses data generated from num_walks 2D random walks. Each walk starts from the center of the canvas (0,0) and takes num_steps steps. The step for each walk is generated using rnorm() for both x and y increments with a standard deviation of step_size_sd, ensuring a degree of unpredictability in their paths.\nPlotting with ggplot2: The ggplot2 package is the primary tool.\n\ngeom_path(): Each random walk is rendered as a separate path. By using geom_path for each of the num_walks random walks, we trace the journey of each data series as a continuous line, creating the filamentous structure. The linewidth is set to 0.3 for thin lines.\nAesthetics & Color:\n\nalpha (transparency): Each path is drawn with alpha = 0.15. Setting a low alpha value for the paths allows their overlaps to create areas of greater density and visual weight, highlighting regions where many paths converge or cross.\ncolor: The color of each line is mapped to its walk_id using factor(walk_id). A repeating spectral palette from RColorBrewer (brewer.pal(11, \"Spectral\")) is used. By mapping the color aesthetic to the unique ID of each random walk using a repeating spectral palette, a vibrant differentiation between lines is achieved, preventing a monotonous visual and adding depth.\n\nTheme and Axes:\n\ntheme_void(): This theme is used to remove axes, gridlines, and most background elements. Employing theme_void() strips the canvas of traditional chart elements, focusing the viewer’s attention entirely on the artistic form of the data itself.\nplot.background = element_rect(fill = \"black\", color = \"black\"): The plot background is set to black to make the colored lines stand out.\ncoord_fixed(): This ensures that the aspect ratio is maintained, so the random walks spread evenly in all directions. Using coord_fixed() maintains a 1:1 aspect ratio, preventing distortion and ensuring the ‘explosion’ of lines feels symmetrical and balanced in its spread.\nlegend.position = \"none\": The legend is hidden to maintain a clean, artistic look."
  },
  {
    "objectID": "541portfolio/2025-06-02-lab-09/index.html#art-piece-2",
    "href": "541portfolio/2025-06-02-lab-09/index.html#art-piece-2",
    "title": "Lab 9: Generative Art",
    "section": "Art Piece 2",
    "text": "Art Piece 2\n\n# Parameters for Perlin noise\ngrid_width &lt;- 500\ngrid_height &lt;- 300\nnoise_frequency &lt;- 0.02 # Lower frequency for larger, smoother patterns\nnoise_octaves &lt;- 5 # More octaves for more detail/richness\n\n# Generate Perlin noise\n# Using long_grid for a more convenient format for ggplot\nnoise_data &lt;- long_grid(\n  x = seq(0, 1, length.out = grid_width),\n  y = seq(0, 1, length.out = grid_height)\n) %&gt;%\n  mutate(\n    noise_value = gen_perlin(x, y, frequency = noise_frequency, octaves = noise_octaves)\n  )\n\n# Create the plot\nggplot(noise_data, aes(x = x, y = y, fill = noise_value)) +\n  geom_raster(interpolate = TRUE) + # interpolate = TRUE for smoother transitions\n  scale_fill_gradientn(\n    colors = c(\"#000033\", \"#000066\", \"#2E0854\", \"#4B0082\", \"#8A2BE2\", \"#FF00FF\", \"#FF69B4\", \"#1E90FF\", \"#00BFFF\", \"#AFEEEE\"),\n    name = \"\" # No legend title\n  ) +\n  theme_void() +\n  coord_fixed(expand = FALSE) + # expand = FALSE to fill the entire plot area\n  theme(\n    legend.position = \"none\", # Hide legend for purely artistic view\n    plot.background = element_rect(fill = \"black\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nTitle: Rstrology\nMuseum-Style Description:\nBehold Rstrology, a sublime confluence where the rigorous logic of R code meets the enigmatic allure of the cosmos, unfurling as a digital aurora of celestial gradients. Bands of vibrant, otherworldly light—deep cosmic indigos melting into spectral purples, superheated magentas, and ethereal cyans—streak across the canvas, suggesting distant nebulae or the very fabric of a synthetic universe taking form. The artist harnesses R to sculpt new firmaments, inviting contemplation on whether these are the birth colors of stars, the resonant frequencies of data streams, or a horoscope cast in code. Rstrology challenges us to find the poetic in the programmatic, seeking the soul in the machine and the universal patterns that bind the stars to the silicon chip, offering a meditation on the inherent beauty found in the structured randomness governing both galaxies and algorithms.\nDescription of the Code Choices and How They Influenced the Appearance:\n\nCore Generative Technique: Perlin Noise: The foundation of this piece is 2D Perlin noise generated using the ambient package in R.\n\ngen_perlin(): A 2D grid of Perlin noise values is created using ambient::gen_perlin() on a grid generated by ambient::long_grid(). The grid dimensions are grid_width by grid_height. By generating a matrix of Perlin noise, we obtain a set of smoothly varying values that form the fundamental structure of the image’s light and dark regions.\nFrequency/Scale: The frequency parameter in gen_perlin() is set to noise_frequency. Choosing a relatively low frequency for the Perlin noise results in large, sweeping patterns and gentle gradations rather than fine, chaotic detail, contributing to the sense of vastness.\nOctaves: The octaves parameter is set to noise_octaves. Using multiple octaves adds layers of noise at different frequencies, creating a richer and more detailed texture within the larger forms.\n\nColor Mapping with ggplot2:\n\ngeom_raster(): The noise matrix is visualized using geom_raster. The interpolate = TRUE argument helps smooth the color transitions between pixels. The geom_raster() function maps each noise value directly to a pixel on the canvas. interpolate = TRUE further enhances the smoothness, creating a more fluid visual field.\nscale_fill_gradientn(): A custom multi-color gradient is used to map noise values to colors. The chosen colors are c(\"#000033\", \"#000066\", \"#2E0854\", \"#4B0082\", \"#8A2BE2\", \"#FF00FF\", \"#FF69B4\", \"#1E90FF\", \"#00BFFF\", \"#AFEEEE\"). By applying this carefully selected multi-color gradient—transitioning from deep blues and purples through vibrant pinks to bright cyans—the raw noise values are transformed into the otherworldly hues that define the piece’s atmosphere.\n\nCanvas Styling:\n\ntheme_void(): This removes all axes, gridlines, and background elements. The use of theme_void() removes all distracting chart elements, immersing the viewer fully in the generated visual.\ncoord_fixed(expand = FALSE): This maintains the aspect ratio of the noise grid and ensures the raster fills the entire plot area without padding. Using coord_fixed(expand = FALSE) ensures the noise patterns are not unnaturally stretched and that the artwork extends to the very edges of the frame.\nlegend.position = \"none\": The color scale legend is hidden.\nplot.background = element_rect(fill = \"black\", color = \"black\"): The plot background is set to black to enhance the luminosity of the colors."
  },
  {
    "objectID": "541portfolio/2025-04-07-lab-01/index.html",
    "href": "541portfolio/2025-04-07-lab-01/index.html",
    "title": "Lab 1: Quarto Warmup & STAT 331 Review",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggridges)"
  },
  {
    "objectID": "541portfolio/2025-04-07-lab-01/index.html#quarto",
    "href": "541portfolio/2025-04-07-lab-01/index.html#quarto",
    "title": "Lab 1: Quarto Warmup & STAT 331 Review",
    "section": "Quarto",
    "text": "Quarto\nFirst, let’s make sure you know how to use Markdown formatting to style a Quarto document.\n\nMake this text bold.\nMake this text italicized.\nMake these into a bullet point list:\n\n\nApples\nBananas\nPotatoes\n\n\nEdit the YAML to remove warning messages from being output in the rendered HTML file\nUsing code chunk options, make it so this chunk shows the plot but not the source code:\n\n\n\n\n\n\n\n\n\n\n\nUsing code chunk options, remove the messages about bandwidth geom_density_ridges() chose to use:\n\n\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_density_ridges() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\n\n\n\n\n\n\n\n\n\nUsing code chunk options, make it so that these plots are printed side-by-side:\n\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_boxplot() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_density_ridges() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\n\n\n\n\n\n\n\n\n\n\n\nUsing code chunk options, make it so this chunk shows the code but not the output:\n\n\n2 + 2\n\n\nUsing code chunk options, make it so the file can still knit even though this chunk has an error\n\n\n2 + a\n\n\nUsing code chunk options, create a descriptive label for each of the code chunks above."
  },
  {
    "objectID": "541portfolio/2025-04-07-lab-01/index.html#data-wrangling-review",
    "href": "541portfolio/2025-04-07-lab-01/index.html#data-wrangling-review",
    "title": "Lab 1: Quarto Warmup & STAT 331 Review",
    "section": "Data Wrangling Review",
    "text": "Data Wrangling Review\nSince you already seen some ggplots, let’s do a bit of review on data handling. In this class, we will exclusively make use of tools from the tidyverse suite of packages to perform our data cleaning and wrangling operations. If you are less familiar with these packages or it’s been some time since you used them, I would strongly recommend referencing the function documentation!\nFor these problems, we will continue to work with the mpg data frame, making various changes to the data to clean it up.\n\nThe fl variable describes the type of fuel for each car, with levels: p, r, e, d, and c. Do some research into what each of these labels mean! Then, use the if_else() function to create a new variable (fuel_type) with two levels: petrol (any car using petrolium-based gas) and alternative energy (any car not using petrolium-based gas).\n\n\nmpg &lt;- mpg %&gt;%\n  mutate(\n    fuel_type = if_else(fl %in% c(\"p\", \"r\"), \n                        \"petrol\", \n                        \"alternative energy\")\n  )\n\n\nThe drv variable describes if the car has front drive (f), rear drive (r), or four wheel drive (4). Let’s make better labels for these values! Specifically, use the case_when() function to change the drv variable to have the following levels: front, rear, four wheel.\n\n\nmpg &lt;- mpg %&gt;%\n  mutate(\n    drv = case_when(\n      drv == \"f\" ~ \"front\",\n      drv == \"r\" ~ \"rear\",\n      drv == \"4\" ~ \"four wheel\"\n    )\n  )\n\n\nThe trans variable contains two pieces of information, (1) the transmission style (auto or manual) and the specific type of transmission (e.g., l5, m5). Using the str_split() function, create a new variable (trans_type) containing the specific type of transmission of each car. Once you’ve made this new variable, use the rename() function to change the name of the trans column to trans_style.\nHint: You will need to deal with the stray parenthesis! (string split + remove extra “)” )\n\n\nmpg &lt;- mpg %&gt;%\n  mutate(\n    # Split on \"(\" and take the second piece (the substring after \"(\")\n    trans_type = str_split(trans, \"\\\\(\", simplify = TRUE)[,2],\n    # Remove the trailing \")\" from that substring\n    trans_type = str_remove(trans_type, \"\\\\)\"),\n    # Split on \"(\" and take the first pierce to get the transmission style\n    trans = str_split(trans, \"\\\\(\", simplify = TRUE)[,1],\n  ) %&gt;%\n  # Rename 'trans' column to 'trans_style'\n  rename(trans_style = trans)"
  },
  {
    "objectID": "541portfolio/2025-04-07-lab-01/index.html#getting-to-know-your-classmates",
    "href": "541portfolio/2025-04-07-lab-01/index.html#getting-to-know-your-classmates",
    "title": "Lab 1: Quarto Warmup & STAT 331 Review",
    "section": "Getting to know your classmates",
    "text": "Getting to know your classmates\n\nFind someone who took Stat 331 from a different professor than you. Compare your experiences. Tell me their name and professor. List one or two things that you think you learned more about, and one or two things that they learned more about.\nLiam took Stat 331 with the queen herself, Em-Dawg. I took it with the Quarto Queen Dr. Theobold. We both had a blast, best class ever. I had it on Zoom, so I probably learned more about learning on Zoom. I learned more about data wrangling, because I wrangled that data. Liam learned that a lot about tidyverse. Samesies. Live, laugh, love tidyverse. Liam learned more about which color pallette are color vision deficient friendly.\nFind someone in the class who does not share your birth month. Tell me their name and birthday, and use R to find out how many days apart your birthdays are.\n\nLily “the GOAT” Cook, birthday is July 11, 2002 (7-eleven)\n\nstart_date &lt;- as.Date(\"2002-07-11\")\nend_date   &lt;- as.Date(\"2002-08-14\")\nnum_days &lt;- end_date - start_date\nnum_days\n\nTime difference of 34 days"
  },
  {
    "objectID": "nhg_ci.html",
    "href": "nhg_ci.html",
    "title": "Research: Negative Hypergeometric Confidence Intervals",
    "section": "",
    "text": "Beginning in Spring 2022, I conducted research on short exact confidence intervals for the parameters of the negative hypergeometric (NHG) distribution. I started this work during my undergraduate studies and continued it through the summer as part of the Frost Summer Undergraduate Research Program (SURP), where I collaborated with Hannah Pawig, who focused on confidence intervals for the Poisson distribution. I later expanded this work into my master’s thesis.\nUnder the mentorship of Dr. Bret Holladay, I developed multiple algorithms to construct optimal confidence intervals for the NHG distribution—intervals that are both valid and more precise than commonly used methods. This work addressed shortcomings in existing approaches, such as undercoverage and overly conservative intervals. To improve accessibility, I created a Shiny app and developed an R package for practitioners and researchers. Dr. Holladay and I are now in the process of preparing a manuscript for submission to a peer-reviewed journal, and my thesis is currently being published through the university."
  },
  {
    "objectID": "nhg_ci.html#thesis-presentation-slides",
    "href": "nhg_ci.html#thesis-presentation-slides",
    "title": "Research: Negative Hypergeometric Confidence Intervals",
    "section": "Thesis Presentation Slides",
    "text": "Thesis Presentation Slides\nThese slides were part of my public master’s thesis presentation. They introduce the NHG distribution and its applications (such as estimating population size via capture-recapture), and describe the motivation, methodology, and comparative performance of the new confidence interval procedures I developed. The presentation concludes with a demo of the Shiny app and R package.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"nhg_ci/ThesisPresentation.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "nhg_ci.html#frost-surp-poster",
    "href": "nhg_ci.html#frost-surp-poster",
    "title": "Research: Negative Hypergeometric Confidence Intervals",
    "section": "Frost SURP Poster",
    "text": "Frost SURP Poster\nThis poster summarized the research Hannah and I conducted during the FROST summer program. It outlined the limitations of existing confidence interval methods for discrete distributions, introduced the Conditional Minimal Cardinality (CMC) method, and showcased our comparative results. It also emphasized our efforts to make these tools accessible through interactive Shiny apps and open-source code. Out of ten FROST research teams, we were one of two teams selected to represent the program at a college-wide research symposium. We also presented this poster at an additional university-wide research conference, sharing our work with a broader academic audience.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"nhg_ci/Frost_SURP_Discrete_CI_2025_Poster.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "nhg_ci.html#frost-surp-slides",
    "href": "nhg_ci.html#frost-surp-slides",
    "title": "Research: Negative Hypergeometric Confidence Intervals",
    "section": "Frost SURP Slides",
    "text": "Frost SURP Slides\nThese slides accompanied the joint presentation Hannah and I gave at the conclusion of our summer research. We presented them at the Statistics Department Summer Research Showcase, where we shared the motivation, methodology, and applications of our work with faculty and peers. The slides highlight the importance of accessible statistical tools, including our development of Shiny apps for both Poisson and NHG distributions, and outline our future goals of publishing the methods and finalizing the R package.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"nhg_ci/Frost_SURP_Discrete_CI_2025_Slides.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "datafest.html",
    "href": "datafest.html",
    "title": "DataFest",
    "section": "",
    "text": "In Spring 2024, I competed in Cal Poly’s first in-person DataFest, a 48-hour data analysis competition hosted by the Statistics Department. My team of four, The R-sonists, was challenged to analyze a real-world dataset provided by CourseKata, an organization focused on improving statistics education. We were tasked with uncovering actionable insights and delivering a concise three-slide presentation to a panel of judges.\nDespite the intense time constraint, we identified key factors impacting student engagement and end-of-course (EOC) outcomes, using multivariate regression and MANOVA to support our recommendations. Our findings emphasized how specific question types and chapter sequencing could better support student learning outcomes.\nOur team was honored with the Best in Show award (out of 10 teams) for the strength of our analysis and presentation. The following year, I was invited back to serve as a DataFest mentor, supporting new participants as a graduate student.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"datafest/DataFest_Presentation.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "datafest.html#team-r-sonists-c.r.a.k.ing-the-code-to-improve-student-learning",
    "href": "datafest.html#team-r-sonists-c.r.a.k.ing-the-code-to-improve-student-learning",
    "title": "DataFest",
    "section": "",
    "text": "In Spring 2024, I competed in Cal Poly’s first in-person DataFest, a 48-hour data analysis competition hosted by the Statistics Department. My team of four, The R-sonists, was challenged to analyze a real-world dataset provided by CourseKata, an organization focused on improving statistics education. We were tasked with uncovering actionable insights and delivering a concise three-slide presentation to a panel of judges.\nDespite the intense time constraint, we identified key factors impacting student engagement and end-of-course (EOC) outcomes, using multivariate regression and MANOVA to support our recommendations. Our findings emphasized how specific question types and chapter sequencing could better support student learning outcomes.\nOur team was honored with the Best in Show award (out of 10 teams) for the strength of our analysis and presentation. The following year, I was invited back to serve as a DataFest mentor, supporting new participants as a graduate student.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"datafest/DataFest_Presentation.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "stat_ed_frost.html",
    "href": "stat_ed_frost.html",
    "title": "Research: Statistics Education",
    "section": "",
    "text": "My research in statistics education focused on improving student engagement in asynchronous online statistics courses, particularly through the development and evaluation of a collaborative learning tool called Collaborative Keys (CKs). I joined this research project through the Frost Undergraduate Research Program, working under Dr. Anelise Sabbag alongside a team of faculty and students.\nOur work used classroom-based research and the Community of Inquiry framework to study how students interact in online group assignments. We investigated how different CK structures influenced student collaboration, performance, and social presence. I contributed to data analysis, rubric design, coding of interactions, and co-authorship of a peer-reviewed paper. This research aimed to inform best practices for fostering meaningful student-to-student interaction in virtual learning environments."
  },
  {
    "objectID": "stat_ed_frost.html#journal-article",
    "href": "stat_ed_frost.html#journal-article",
    "title": "Research: Statistics Education",
    "section": "Journal Article",
    "text": "Journal Article\nPublished in the Journal of Statistics and Data Science Education, this peer-reviewed article presents our research on revising Collaborative Keys to better promote engagement and cooperative learning. We found that moving from whole-class to small-group CKs led to more meaningful interaction, stronger social presence, and improved collaboration. I contributed to both the research and authorship of the paper, which showcases empirical evidence and offers practical implications for instructors designing online statistics courses.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"stat_ed_frost/The Development of Collaborative Keys to Promote Engagement in Undergraduate Online Asynchronous Statistics Courses.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "stat_ed_frost.html#frost-surp-poster",
    "href": "stat_ed_frost.html#frost-surp-poster",
    "title": "Research: Statistics Education",
    "section": "Frost SURP Poster",
    "text": "Frost SURP Poster\nThis poster was presented at a college-wide research conference as part of our participation in the Frost Undergraduate Research Program. It summarizes our research questions, methods, and early findings on how Collaborative Keys (CKs) foster student engagement in online statistics courses. The poster highlights trends in student performance and interaction patterns, outlines our grading rubric, and discusses how different CK structures influenced group dynamics and participation. We also introduced our use of the Community of Inquiry framework to evaluate social, cognitive, and teaching presence.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"stat_ed_frost/poster_FROST.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "stat_ed_frost.html#frost-surp-presentation",
    "href": "stat_ed_frost.html#frost-surp-presentation",
    "title": "Research: Statistics Education",
    "section": "Frost SURP Presentation",
    "text": "Frost SURP Presentation\nThese slides were presented at the Statistics Department Summer Research Colloquium and walk through our research process in more detail. The presentation covered our rubric development, coding of student responses, and use of the Community of Inquiry framework. We shared insights from our literature review and preliminary analysis, and described how small-group CKs led to increased collaboration and improved peer interaction in asynchronous learning environments.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"stat_ed_frost/FROST_Fall_Slides.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "I’m actively seeking full-time opportunities in data science or analytics and am open to relocating anywhere within the U.S. My background combines strong statistical foundations, programming skills, and applied research experience, and I’m excited to bring these strengths to a collaborative, impact-driven team. Below is my resume.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"resume/Resume_Rachel_Roggenkemper.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rachel Roggenkemper",
    "section": "",
    "text": "Hi, I’m Rachel! I recently earned my Master of Science in Statistics from Cal Poly San Luis Obispo, where I also completed a Data Science minor through the university’s blended BS/MS program. My academic and professional experiences reflect a passion for statistical problem-solving, data storytelling, and collaborative work across disciplines. I was honored to receive both the Outstanding Graduate Student Award at the college and department level, as well as the Community Leadership Award, recognizing both academic excellence and meaningful contributions to the Cal Poly community.\nDuring my time at Cal Poly, I led and contributed to several research and consulting initiatives—from developing short exact confidence intervals for the negative hypergeometric distribution to working with real clients in agriculture, life sciences, and engineering through the university’s graduate consulting practicum. I also co-authored a peer-reviewed publication on statistics education as a Frost Research Scholar and developed an AI-powered image analysis tool for the Global Emancipation Network to support anti-trafficking efforts.\nProfessionally, I gained hands-on experience as a Data Science Co-op at Guy Carpenter, where I built scalable dashboards in R, automated hurricane event response reporting, and validated key data inputs for catastrophe risk models. I’ve also supported over 1,000 students as a grader and lab assistant for 13 different statistics courses, helping reinforce statistical thinking and coding in R.\nBeyond academia and data science, I hold a California Real Estate License and maintain a strong interest in the real estate sector, particularly where it intersects with data and housing analytics. I’m especially passionate about data visualization and take pride in crafting dashboards and analyses that are both rigorous and accessible to non-technical audiences.\nOutside of work and study, I’m an avid traveler and sports enthusiast. I love skiing, pickleball, Formula 1, and soccer—all of which keep me grounded and inspired. I’m currently seeking full-time opportunities in data science, analytics, or applied research, and I’m open to relocating anywhere in the U.S."
  },
  {
    "objectID": "541portfolio/2025-05-27-lab-08/index.html",
    "href": "541portfolio/2025-05-27-lab-08/index.html",
    "title": "Lab 8: Web Scraping",
    "section": "",
    "text": "Goal: Scrape information from https://www.cheese.com to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. 🪤\n\nPart 1: Locate and examine the robots.txt file for this website. Summarize what you learn from it.\nThe robots.txt is located at https://www.cheese.com/robots.txt . This file is pretty blank, with only two lines. These two lines tell us that anyone can crawl (User-agent is set to *) and where the site map is located (https://www.cheese.com/sitemap.xml)\nPart 2: Learn about the html_attr() function from rvest. Describe how this function works with a small example.\nThe html_attr() function gets and returns a character vector containing the values of the specified attribute for an element of the HTML file (ex. href, class, id). This function requires two arguments; x, the HTML element (usually from read_html()), name, the name of the attribute to retrieve, like href, class, etc. There is a third optional argument, default, which is related to how non-existent attributes are handled, which defaults to NA_character, meaning NA is returned in these cases.\n\n# define url\ncheese_url &lt;- \"https://www.cheese.com\"\n\n# fetch html content \nhtml_doc_cheese &lt;- read_html(cheese_url) # note: in the real world, add error handling\n\n# select all &lt;a&gt; tags on page\nall_links_nodes &lt;- html_nodes(html_doc_cheese, \"a\")\n\n# use html_attr() to extract the 'href' attribute from each link\nall_hrefs_cheese &lt;- html_attr(all_links_nodes, \"href\")\n\nhead(all_hrefs_cheese, 10)\n\nPart 3: (Do this alongside Part 4 below.) I used ChatGPT to start the process of scraping cheese information with the following prompt:\n\nWrite R code using the rvest package that allows me to scrape cheese information from cheese.com.\n\nFully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.\n\n# load libraries\nlibrary(rvest)\nlibrary(dplyr)\n\n# define url\nurl &lt;- \"https://www.cheese.com/alphabetical\"\n\n# read html content from the page\nwebpage &lt;- read_html(url)\n\n# extract cheese names and urls\ncheese_data &lt;- webpage %&gt;%\n  html_nodes(\".cheese-item\") %&gt;%\n  html_nodes(\"a\") %&gt;%\n  html_attr(\"href\") %&gt;%\n  paste0(\"https://cheese.com\", .)\n\ncheese_names &lt;- webpage %&gt;%\n  html_nodes(\".cheese-item h3\") %&gt;%\n  html_text()\n\n# create df to store results\ncheese_df &lt;- data.frame(Name = cheese_names,\n                        URL = cheese_data,\n                        stringsAsFactors = FALSE)\n\nprint(cheese_df)\n\nNot useful:\nThere were many empty results. Both cheese_data (for URLs) and cheese_names (for cheese names) were empty character vectors. Because of this, the cheese_df dataframe was also empty.\nThe selectors were not specific enough. The CSS selectors .cheese-item and .cheese-item h3 suggested by ChatGPT were too generic/ did not accurately reflect the current structure of the cheese.com/alphabetical page. Websites frequently update their structure, and AI might be trained on old versions or make incorrect assumptions about common class names.\nUseful:\nChatGPT provided a basic template of rvest functions (read_html, html_nodes, html_attr, html_text) which was conceptually helpful for recalling the workflow.\nPart 4: Obtain the following information for all cheeses in the database:\n\ncheese name\nURL for the cheese’s webpage (e.g., https://www.cheese.com/gouda/)\nwhether or not the cheese has a picture (e.g., gouda has a picture, but bianco does not).\n\nTo be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\n\n# defining url and pages to look at\nbase_url &lt;- \"https://www.cheese.com/alphabetical/?per_page=100\"\npage_numbers &lt;- 1:21\n\n# Helper function to extract text or attributes based on tag structure\nextract_info &lt;- function(page, outer_selector, inner_selector, attr = NULL) {\n  nodes &lt;- page %&gt;%\n    html_elements(outer_selector) %&gt;%\n    html_elements(inner_selector)\n  \n  if (!is.null(attr)) { #dealing with null values\n    html_attr(nodes, attr)\n  } else {\n    html_text(nodes)\n  }\n}\n\n# Function to scrape a single page\nscrape_cheese_page &lt;- function(page_number) {\n  full_url &lt;- paste0(base_url, \"&page=\", page_number)\n  page &lt;- read_html(full_url)\n\n  data.frame(\n    Name = extract_info(page, \"div.product-item\", \"h3\"), #cheese name\n    url = paste0(\"https://www.cheese.com\", extract_info(page, \"div.product-item\", \"h3 a\", \"href\")), #cheese url\n    whether = extract_info(page, \"div.product-item\", \"img\", \"class\"), #if there is image\n    stringsAsFactors = FALSE\n  )\n}\n\n# Map over all pages and bind results\ncheese_data &lt;- map_dfr(page_numbers, function(pg) {\n  result &lt;- scrape_cheese_page(pg)\n  Sys.sleep(1)  # delay to be nice\n  result\n})\n\nhead(cheese_data)\n\n                               Name\n1           2 Year Aged Cumin Gouda\n2            3-Cheese Italian Blend\n3 30 Month Aged Parmigiano Reggiano\n4           3yrs Aged Vintage Gouda\n5                        Aarewasser\n6                  Abbaye de Belloc\n                                                             url       whether\n1                https://www.cheese.com/2-year-aged-cumin-gouda/  image-exists\n2                 https://www.cheese.com/3-cheese-italian-blend/ image-missing\n3 https://www.cheese.com/30-month-aged-parmigiano-reggiano-150g/  image-exists\n4                https://www.cheese.com/3yrs-aged-vintage-gouda/  image-exists\n5                             https://www.cheese.com/aarewasser/  image-exists\n6                       https://www.cheese.com/abbaye-de-belloc/  image-exists\n\n\nPart 5: When you go to a particular cheese’s page (like gouda), you’ll see more detailed information about the cheese. For just 10 of the cheeses in the database, obtain the following detailed information:\n\nmilk information\ncountry of origin\nfamily\ntype\nflavour\n\n(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)\n\nextract_text &lt;- function(page, selector) {\n  page %&gt;%\n    html_elements(selector) %&gt;%\n    html_text()\n}\n\n# Scrape cheese detail from a single page URL\nscrape_cheese_details &lt;- function(url) {\n  Sys.sleep(1)  # delay to be nice\n  \n  page &lt;- read_html(url)\n  \n  tibble(\n    family = extract_text(page, \".summary_family p\"),\n    milk = extract_text(page, \".summary_milk p\"),\n    country_of_origin = extract_text(page, \".summary_country p\"),\n    type = extract_text(page, \".summary_moisture_and_type p\"),\n    flavour = extract_text(page, \".summary_taste p\")\n  )\n}\n\ncheeses &lt;- c(\n    \"Gouda\", \"Colby\", \"Applewood\",\n    \"Vacherin\", \"Pecorino Romano\",\n    \"Cornish Blue\", \"Camembert\", \n    \"Stella Feta\", \"Dubliner\", \"Paneer\"\n  )\n# Select cheese URLs of interest\ncheese_urls &lt;- cheese_data %&gt;%\n  filter(Name %in% cheeses) %&gt;%\n  pull(url)\n\n# Map and combine all details into a single tibble\ndf_cheeses &lt;- map_dfr(cheese_urls, scrape_cheese_details)\n\n# cleaning df for readibility\ndf_cheeses &lt;- df_cheeses %&gt;%\n  mutate( # removing unnecessary labels in vars\n    family = str_remove(family, 'Family: '),\n    milk = str_remove(milk, 'Made from '),\n    country_of_origin = str_remove(country_of_origin, 'Country of origin: '),\n    type = str_remove(type, 'Type: '),\n    flavour = str_remove(flavour, \"Flavour: \")\n  )\n\nnames &lt;- cheese_data %&gt;%\n  filter(Name %in% cheeses) %&gt;%\n  select(Name)\n\ndf_cheeses &lt;- cbind(names, df_cheeses)\n\ndf_cheeses\n\n              Name    family\n1        Applewood   Cheddar\n2        Camembert Camembert\n3            Colby   Cheddar\n4     Cornish Blue      Blue\n5         Dubliner   Cheddar\n6            Gouda     Gouda\n7           Paneer   Cottage\n8  Pecorino Romano  Pecorino\n9      Stella Feta      Feta\n10        Vacherin      Brie\n                                                         milk\n1                                      pasteurized cow's milk\n2                                                  cow's milk\n3                                                  cow's milk\n4                                      pasteurized cow's milk\n5                                      pasteurized cow's milk\n6  pasteurized or unpasteurized cow's, goat's or sheep's milk\n7                   pasteurized cow's or water buffalo's milk\n8                                                sheep's milk\n9                                      pasteurized cow's milk\n10                                     pasteurized cow's milk\n        country_of_origin               type       flavour\n1                 England          semi-hard       smokey \n2                  France      soft, artisan         sweet\n3           United States          semi-hard         sweet\n4                 England semi-soft, artisan creamy, sweet\n5                 Ireland               hard  nutty, sweet\n6             Netherlands               hard full-flavored\n7    Bangladesh and India         fresh firm         milky\n8                   Italy               hard  salty, sharp\n9           United States      firm, artisan         tangy\n10 France and Switzerland      soft, artisan        smooth\n\n\nPart 6: Evaluate the code that you wrote in terms of efficiency. To what extent do your function(s) adhere to the principles for writing good functions? To what extent are your functions efficient? To what extent is your iteration of these functions efficient?\nThe functions we wrote follow the principles of good function design by being modular, clear, and reusable. Each function performs a single responsibility. For example, extract_info() and scrape_cheese_details() are each focused on one task. Using purrr::map_dfr() improves efficiency over for loops by combining iteration and row-binding in a memory-friendly way, avoiding repeated rbind() calls that can slow down execution. While Sys.sleep(1) adds intentional delay, it’s necessary for responsible scraping and to be nice to the website."
  },
  {
    "objectID": "541portfolio/2025-04-14-lab-02/index.html",
    "href": "541portfolio/2025-04-14-lab-02/index.html",
    "title": "Lab 2: Advanced Data Visualization",
    "section": "",
    "text": "Create a Quarto file for ALL Lab 2 (no separate files for Parts 1 and 2).\n\nMake sure your final file is carefully formatted, so that each analysis is clear and concise.\nBe sure your knitted .html file shows all your source code, including any function definitions.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(here)\nlibrary(scales)\nlibrary(plotly)\nlibrary(leaflet)\nlibrary(maps)\nlibrary(sf)\nlibrary(geojsonio)"
  },
  {
    "objectID": "541portfolio/2025-04-14-lab-02/index.html#dissecting-a-bad-visualization",
    "href": "541portfolio/2025-04-14-lab-02/index.html#dissecting-a-bad-visualization",
    "title": "Lab 2: Advanced Data Visualization",
    "section": "Dissecting a Bad Visualization",
    "text": "Dissecting a Bad Visualization\nBelow is an example of a less-than-ideal visualization from the collection linked above. It comes to us from data provided for the Wellcome Global Monitor 2018 report by the Gallup World Poll:\n\n\nWhile there are certainly issues with this image, do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nThis graph tells the story of the world regions and their level of belief that vaccines are safe by each country in that region. It is trying to show the regions that have the greatest and least belief that vaccines are safe, and shows within that region, which countries have the greatest and least belief in vaccines.\nThe authors mean to convey the region’s overall belief in vaccines, in addition to specific countries in that region. With the inclusion of the regional medians, we can compare the regions against each other. We can also compare the countries in that region against their regional median.\nIn terms of regional medians, we learn that Asia has the highest belief in vaccines while the Former Soviet Union has the lowest belief in vaccines.\nList the variables that appear to be displayed in this visualization. Hint: Variables refer to columns in the data.\n\nGlobal region\nCountry\nPercentage of people who believe in vaccines\n\nNow that you’re versed in the grammar of graphics (e.g., ggplot), list the aesthetics used and which variables are mapped to each.\n\nx-axis: percentage of people who believe in vaccines\ny-axis: country\ncolor: region\nfacet: region\ntext labels: all regions and some countries (low and high within regions)\nvertical lines: regional medians\n\nWhat type of graph would you call this? Meaning, what geom would you use to produce this plot?\ngeom_point()\nProvide at least four problems or changes that would improve this graph. Please format your changes as bullet points!\n\n\nFix the layout (y-axis), right now it doesn’t provide useful information about the countries so remove the y-axis\nRemove the legend, it is just providing repeat information\nProvide some consistency along the country labels, right now it seems like they are being randomly labeled within each region\nChange the “%” in the title to “Percentage”\nAdd “%” to the x-axis labels"
  },
  {
    "objectID": "541portfolio/2025-04-14-lab-02/index.html#improving-the-bad-visualization",
    "href": "541portfolio/2025-04-14-lab-02/index.html#improving-the-bad-visualization",
    "title": "Lab 2: Advanced Data Visualization",
    "section": "Improving the Bad Visualization",
    "text": "Improving the Bad Visualization\nThe data for the Welcome Global Monitor 2018 report can be downloaded at the following site: https://wellcome.ac.uk/reports/wellcome-global-monitor/2018\n\nThere are two worksheets in the downloaded dataset file. You may need to read them in separately, but you may also just use one if it suffices.\n\n# Loading in full WGM dataset from Sheet 2 \nwgm2018_full &lt;- read_xlsx(\n  here(\"data\", \"wgm2018-dataset-crosstabs-all-countries.xlsx\"),\n  sheet = 2)\n\n# Extract raw country information from Sheet 3, cell C2 (relates to WP5)\n# Clean and reshape country information into WP5 column and country name column \ncountry_names &lt;- read_xlsx(\n  here(\"data\", \"wgm2018-dataset-crosstabs-all-countries.xlsx\"),\n  sheet = 3,\n  range = \"C2:C2\",\n  col_names = \"countries\") %&gt;%\n  mutate(num_country = str_split(countries, pattern = \",\")) %&gt;%\n  unnest(num_country) %&gt;%\n  mutate(\n    WP5 = as.numeric(\n      str_split(num_country, pattern = \"=\", simplify = TRUE)[, 1]\n      ),\n    country = str_split(num_country, pattern = \"=\", simplify = TRUE)[, 2]\n    ) %&gt;%\n  select(WP5, country) %&gt;%\n  filter(!is.na(WP5))\n\n# Extra raw region information from Sheet 3, cell C58 (relates to Regions_Report)\n# Clean and reshape region information into Regions_Report column and region name column \nregion_names &lt;- read_xlsx(\n  here(\"data\", \"wgm2018-dataset-crosstabs-all-countries.xlsx\"),\n  sheet = 3,\n  range = \"C58\",\n  col_names = \"regions\") %&gt;%\n  mutate(num_region = str_split(regions, \",\")) %&gt;%\n  unnest(num_region) %&gt;%\n  mutate(\n    Regions_Report = as.numeric(str_split(num_region, \"=\", simplify = TRUE)[,1]),\n    region = str_split(num_region, \"=\", simplify = TRUE)[,2]\n    ) %&gt;%\n  select(Regions_Report, region) %&gt;%\n  filter(!is.na(Regions_Report))\n\n# Join country and region information with full WGM data\nwgm2018_full &lt;- wgm2018_full %&gt;%\n  left_join(country_names, by = \"WP5\") %&gt;%\n  left_join(region_names, by = \"Regions_Report\")\n\n# Create global region variables (collapsed grouping)\nwgm2018_full &lt;- wgm2018_full %&gt;%\n  mutate(\n    global_region = case_when(\n      country %in% c(\"Estonia\", \"Latvia\", \"Lithuania\") ~ \"Former Soviet Union\",\n      country %in%\n        c(\"Bulgaria\", \"Czech Republic\", \"Hungary\",\n          \"Poland\", \"Romania\", \"Slovakia\") ~ \"Europe\",\n      region %in% c(\"Middle East\", \"North Africa\") ~\n        \"Middle East and North Africa\",\n      region %in% c(\"Central Asia\", \"Eastern Europe\")  ~ \"Former Soviet Union\",\n      str_detect(region, \"America\") ~ \"Americas\",\n      str_detect(region, \"Africa\") ~ \"Africa\",\n      str_detect(region, \"Asia\") | region == \"Aus/NZ\" ~ \"Asia\",\n      TRUE ~ \"Europe\"   # include \"Not Assigned\"\n    )\n  )\n\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\nvaccine_data &lt;- wgm2018_full %&gt;%\n  select(global_region, country, Q25) %&gt;%\n  mutate(vaccine_safe = case_when(\n    Q25 %in% 1:2 ~ 1,  # strongly/somewhat agree\n    Q25 %in% 3:5 ~ 0,  # neutral or disagree\n    TRUE ~ NA_real_   # don't know / refused\n    )) %&gt;%\n  # remove don't know / refused\n  filter(vaccine_safe &lt;= 1) %&gt;%   \n  group_by(country, global_region) %&gt;%\n  # country averages\n  summarise(vaccine_safe = mean(vaccine_safe)) %&gt;%\n  group_by(global_region) %&gt;%\n  # regional medians\n  mutate(\n    region_median = median(vaccine_safe),\n    global_region = fct_reorder(as.factor(global_region), desc(region_median)),\n    min_max = case_when(\n      vaccine_safe == max(vaccine_safe) ~ country,\n      vaccine_safe == min(vaccine_safe) ~ country)\n  )  \n  \nvaccine_data %&gt;%\n  ggplot(aes(y = reorder(global_region, region_median), \n             x = vaccine_safe, \n             color = global_region)) +\n    geom_point(aes(alpha = 0.78, \n                   size = 3)) +\n    geom_errorbar(aes(y = global_region, \n                      xmax = region_median, \n                      xmin = region_median),\n                  size = 0.5, \n                  linetype = \"solid\", \n                  width = 1, \n                  color = \"black\") +\n    scale_color_manual(values = c(\"skyblue1\", \"seagreen4\", \"yellow2\",\n                                  \"orangered4\", \"salmon1\", \"dodgerblue4\")) +\n    geom_linerange(aes(xmin = region_median, \n                       xmax = region_median,\n                       group = global_region)) +\n    labs(x = NULL, \n         y = NULL,\n         title = \"Percentage of People who Believe Vaccines are Safe,\n         \\nby Country and Global Region\") +\n    theme_bw() +\n    theme(legend.position = \"none\",\n          panel.grid.major.y = element_blank(),\n          panel.grid.minor.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.y = element_blank(),\n          plot.title = element_text(face = 'bold')) +\n    scale_x_continuous(breaks = seq(0.2, 1, by = 0.1),\n                       labels = label_percent(),\n                       sec.axis = dup_axis()) +\n    guides(color = FALSE) +\n    geom_text(aes(y = global_region, x = 0.2, label = global_region), \n              vjust = -1,\n              hjust = 0,\n              size = 4,\n              fontface = \"bold\") +\n    geom_text(aes(label = min_max),\n                    vjust = 0,\n                    hjust = 0,\n                    size = 3,\n                    color = \"gray18\") +\n    annotate(geom = \"text\",\n             x = 0.865, \n             y = \"Asia\", \n             label = \"Regional Median\",\n             color = \"black\", \n             size = 2.5,\n             hjust = 0,\n             vjust = -3)"
  },
  {
    "objectID": "541portfolio/2025-04-14-lab-02/index.html#second-data-visualization-improvement",
    "href": "541portfolio/2025-04-14-lab-02/index.html#second-data-visualization-improvement",
    "title": "Lab 2: Advanced Data Visualization",
    "section": "Second Data Visualization Improvement",
    "text": "Second Data Visualization Improvement\nFor this second plot, you must select a plot that uses maps so you can demonstrate your proficiency with the leaflet package!\n\nSelect a data visualization in the report that you think could be improved. Be sure to cite both the page number and figure title. Do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\n\n\nChart 2.14: Map of interest in knowing more about medicine, disease or health by country (page 39)\nThis graph is telling the reader the percentage of interest of people who would like to know more about medicine, disease or health by country. The map is shaded using a different shade of green for based on the interest level of each country, a darker shade of green representing more interest.\n\nList the variables that appear to be displayed in this visualization.\n\nThe variables that appear to be displayed in this visualization are the countries and people in the country’s interest level in whether they would like to know more about medicine, disease, or health.\n\nNow that you’re versed in the grammar of graphics (ggplot), list the aesthetics used and which variables are specified for each.\n\nI wouldn’t use ggplot to remake this plot. I would use leaflet to map the people in the country’s interest level in whether they would like to know more about medicine, disease, or health to their respective country in the map, shading it using a green color scale that represents the interest level of a country, a darker shade of green representing more interest.\n\nWhat type of graph would you call this?\n\nI would call this type of graph a choropleth map.\n\nList all of the problems or things you would improve about this graph.\n\nMy main problem with this graph is the coloring. The graph just uses one color green, and the shades of green are very similar, so it’s difficult to tell the differences between the countries. Thus, I would use a more diverse color palette to fix this issue. Something I would improve about this graph is add an interactive element so when you hover above the country, you can see the exact percentage of interest level.\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\n# Compiling interest in knowing more about medicine, disease or health data \nhealth_data &lt;- wgm2018_full %&gt;%\n  select(country, Q7) %&gt;%\n  filter(Q7 %in% c(1, 2)) %&gt;%\n  mutate(conf = case_when(\n    Q7 == 2 ~ 0,\n    Q7 == 1 ~ 1)) %&gt;%\n  group_by(country) %&gt;%\n  summarise(average = mean(conf, na.rm = TRUE)) %&gt;%\n  mutate(percentage = average * 100) %&gt;%\n  select(country, percentage)\n\n\n# Load GeoJSON data of all countries\nworld_geojson &lt;- geojson_read(\"https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json\", what = \"sp\")\n\nworld_geojson &lt;- world_geojson %&gt;%\n  st_as_sf() %&gt;%\n  rename(country = name)\n\n# Combining health data and GeoJSON data\nhealth_data_json &lt;- world_geojson %&gt;%\n  st_as_sf() %&gt;%\n  left_join(health_data, by = \"country\") \n\n# Leaflet Map\npallatte &lt;- colorNumeric(\"plasma\", domain = health_data_json$percentage)\n\nleaflet(health_data_json) %&gt;%\n  setView(lng = 0, lat = 0, zoom = 1) %&gt;%\n  addPolygons(stroke = FALSE, \n              smoothFactor = 0.2, \n              fillOpacity = 1,\n              color = ~pallatte(percentage),\n              label = paste0(health_data_json$country, \": \", \n                             round(health_data_json$percentage, 1), \"%\"),\n              highlightOptions = highlightOptions(weight = 5,\n                                                  color = \"#000000\",\n                                                  fillOpacity = 0.7)) %&gt;%\n  addLegend(pallatte, \n            values = ~percentage,\n            opacity = 0.8,\n            title = \"Interest level (%)\",\n            position = \"bottomleft\",\n            labFormat = labelFormat(suffix = \"%\")) %&gt;%\n  addControl(html = \"&lt;div class='map-title'&gt;Map of interest in learning more about medicine,&lt;br&gt;disease, or health by country&lt;/div&gt;\",\n             position = \"topright\")"
  },
  {
    "objectID": "541portfolio/2025-04-14-lab-02/index.html#third-data-visualization-improvement",
    "href": "541portfolio/2025-04-14-lab-02/index.html#third-data-visualization-improvement",
    "title": "Lab 2: Advanced Data Visualization",
    "section": "Third Data Visualization Improvement",
    "text": "Third Data Visualization Improvement\nFor this third plot, you must use one of the other ggplot2 extension packages mentioned this week (e.g., gganimate, plotly, patchwork, cowplot).\n\nSelect a data visualization in the report that you think could be improved. Be sure to cite both the page number and figure title. Do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\n\n\nChart 5.4: Scatterplot exploring people’s perceptions of vaccine safety and vaccine effectiveness (page 112)\nThis graph is telling the reader the relationship between people’s perceptions of vaccine safety and vaccine effectiveness. It shows the percentage of a country’s people that disagree that vaccines are safe with the percentage of them who disagree that vaccines are effective. I think the authors are trying to convey the relationship between these two variables, and how they change with one another, whcih is why they aded the yellow trend line. I think they are also trying to highlight the countries that have either an abnormally high level of disagreement of people who think vaccines are safe and vaccines are effective, which is why they have some countries labeled.\n\nList the variables that appear to be displayed in this visualization.\n\nThe variables that appear to be displayed in this visualization are the countries, the percentage of people in the country’s that disagree that vaccines are safe, and the percentage of people in the country’s that disagree that vaccines are effective.\n\nNow that you’re versed in the grammar of graphics (ggplot), list the aesthetics used and which variables are specified for each.\n\n\nx-axis: percentage of people in the country’s that disagree that vaccines are safe\ny-axis: percentage of people in the country’s that disagree that vaccines are effective\ncolor: country (shows in legend)\n\nNot an aesthetic, but there is a geom_point() and a geom_smooth()\n\nWhat type of graph would you call this?\n\nI would call this type of graph a scatterplot.\n\nList all of the problems or things you would improve about this graph.\n\nThe main problem I see with this plot is that you have to tilt your head to read the y-axis label. Something I would improve about this graph is instead of just having some countires labeled, to add an interactive element where if you hover over a point, it gives you the country label along with information about the percentages for that respective country.\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\n# Compiling vaccine safety/effectiveness data \nvaccine_plot &lt;- wgm2018_full %&gt;%\n  select(country, Q25, Q26) %&gt;%\n  mutate(disagree_safe = case_when(Q25 &lt;= 3 ~ 0,\n                                   Q25 &lt;= 5 ~ 1,\n                                   TRUE ~ 0),\n         disagree_effective = case_when(Q26 &lt;= 3 ~ 0,\n                                        Q26 &lt;= 5 ~ 1,\n                                        TRUE ~ 0)) %&gt;%\n  group_by(country) %&gt;%\n  summarise(across(c(disagree_safe, disagree_effective), mean, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = disagree_safe, y = disagree_effective)) +\n  geom_point(aes(text = paste0(\"Country: \", country, \"&lt;br&gt;\",\n                               \"Disagree vaccines are safe: \", \n                               round(disagree_safe * 100, 2), \"%&lt;br&gt;\",\n                               \"Disagree vaccines are effective: \", \n                               round(disagree_effective * 100, 2), \"%\")),\n             color = \"skyblue1\", shape = 15, size = 1.8) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"yellow2\", linewidth = 0.8) +\n  theme_bw() +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1),\n                     labels = label_percent()) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1),\n                     labels = label_percent()) +    \n  labs(x = NULL,\n       y = NULL)\n\n# Turn into interactive plotly plot\nggplotly(vaccine_plot, tooltip = \"text\") %&gt;%\n  layout(title = list(text = paste0(\"Scatterplot exploring people's perceptions of vaccine safety &lt;br&gt; and vaccine effectiveness\",\n                                    \"&lt;br&gt;&lt;sup&gt;Percentage of people who disagree that vaccines are safe by percentage of people who &lt;br&gt; disagree that vaccines are effective&lt;/sup&gt;\"),\n                      x = 0.01,  \n                      xanchor = \"left\"),\n         margin = list(t = 150))"
  },
  {
    "objectID": "course_projects.html",
    "href": "course_projects.html",
    "title": "Course Projects",
    "section": "",
    "text": "This section showcases a range of projects completed throughout my undergraduate and graduate coursework in statistics, data science, and computer science. Each post corresponds to a specific course and highlights hands-on work applying analytical methods, computational tools, and statistical theory to real-world data problems.\nProjects span topics such as machine learning, generalized linear models, survival analysis, experimental design, stochastic processes, distributed computing, and statistics education. Whether developing predictive models, analyzing sequential processes, or building interactive reports, each project reflects the applied and interdisciplinary focus of my academic experience.\nClick any course below to explore the work in more detail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDATA 403: Data Science Projects Laboratory\n\n\n\nData Science\n\n\nPython\n\n\nR\n\n\n\nProjects involving comparison of predictive and interpretable regression models, implementing linear classifiers with gradient descent, implementing neural networks from…\n\n\n\nRachel Roggenkemper\n\n\nSep 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSC 466: Knowledge Discovery from Data\n\n\n\nComputer Science\n\n\nPython\n\n\n\nOverview of modern knowledge discovery from data (KDD) methods and technologies. Topics in data mining (association rules mining, classification, clustering), information…\n\n\n\nRachel Roggenkemper\n\n\nSep 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 423: Design and Analysis of Experiments II\n\n\n\nStatistics\n\n\nJMP\n\n\n\n2k factorial designs, 3k factorial designs, balanced and partially balanced incomplete block designs, nested designs, split-plot designs, response surface methodology…\n\n\n\nRachel Roggenkemper\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 417: Survival Analysis Methods\n\n\n\nStatistics\n\n\nMinitab\n\n\n\nParametric and nonparametric methods for analyzing survival data. Topics include Kaplan-Meier and Nelson-Aalen estimates, Cox regression models, accelerated failure time…\n\n\n\nRachel Roggenkemper\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDATA 301: Introduction to Data Science\n\n\n\nData Science\n\n\nPython\n\n\n\nIntroduction to the field of data science and the workflow of a data scientist. Types of data (tabular, textual, sparse, structured, temporal, geospatial), basic data…\n\n\n\nRachel Roggenkemper\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 365: Statistical Communication\n\n\n\nStatistics\n\n\nR\n\n\nExcel\n\n\n\nWritten communication of statistical ideas and content. Analyze data using appropriate methods from previous statistics courses. Writing technical reports with appropriate…\n\n\n\nRachel Roggenkemper\n\n\nApr 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 334: Applied Linear Models\n\n\n\nStatistics\n\n\nR\n\n\n\nLinear models in algebraic and matrix form, diagnostics, transformations, polynomial models, categorical predictors, model selection, correlated errors, logistic regression.\n\n\n\nRachel Roggenkemper\n\n\nApr 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 323: Design and Analysis of Experiments I\n\n\n\nStatistics\n\n\nJMP\n\n\n\nPrinciples, construction and analysis of experimental designs. Completely randomized, randomized complete block, Latin squares, Graeco Latin squares, factorial, and nested…\n\n\n\nRachel Roggenkemper\n\n\nJan 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]